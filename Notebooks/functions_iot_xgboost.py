import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
import xgboost as xgb
from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, roc_curve, roc_auc_score, make_scorer
from scikitplot.metrics import plot_confusion_matrix
import matplotlib.pyplot as plt

#https://medium.com/@silvaan/ensemble-methods-tuning-a-xgboost-model-with-scikit-learn-54ff669f988a

#XGBoost Classification Model with Cross-Validation
def xgboost_class_model(data, 
                        drop_cols=['label'], 
                        test_size=0.33, 
                        seed=[42], 
                        objective=['binary:logistic', 'binary:logitraw'], 
                        booster=['gbtree', 'gblinear', 'dart'],
                        base_score=[0.5],
                        n_estimators=[10, 50, 100], 
                        use_label_encoder=False, 
                        l_r=[0.05, 0.1, 0.25, 0.5, 0.75, 1],
                        eval_metric='mlogloss', 
                        n_fold=3, 
                        num_boost_round=10, 
                        lambda_param=[0, 1, 10, 100],
                        alpha_param=[0, 1, 10, 100], 
                        early_stop_rounds=[5], 
                        max_d=[2, 5, 10, 20, 50],
                        cols_byt_vals=[0.1, 0.3, 0.5, 0.8, 1], 
                        nthread_vals=[4], 
                        min_child_weight_vals=[0, 1, 11, 30, 50, 100], 
                        silent_vals=[1],
                        subsample_vals=[0.1, 0.3, 0.5, 0.8, 0.9], 
                        shuffle=True, 
                        scoring = {'AUC':'roc_auc', 'Accuracy': make_scorer(accuracy_score)}):

    X, y = data.drop(columns=drop_cols), data.label #features and label
    #Create the training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed[0], shuffle=shuffle)
    
    xgb_param_grid = {
        'objective': objective,
        'nthread': nthread_vals, #when use hyperthread, xgboost may become slower
        'learning_rate': l_r,
        'reg_lambda': lambda_param,
        'reg_alpha': alpha_param,
        'max_depth': max_d,
        'n_estimators': n_estimators, #number of trees
        'min_child_weight': min_child_weight_vals,
        'silent': silent_vals,
        'subsample': subsample_vals,
        'colsample_bytree': cols_byt_vals,
        'seed': seed,
        'early_stopping_rounds': early_stop_rounds,
        'booster': booster,
        'base_score': base_score
    }

    #Training the model with parameters selected
    xgb_clf = xgb.XGBClassifier(use_label_encoder=use_label_encoder, eval_metric=eval_metric)
    
    #Choosing 10 different values for the hyperparameters
    grid_error = RandomizedSearchCV(estimator=xgb_clf, param_distributions=xgb_param_grid,
                                    scoring=scoring, cv=n_fold, verbose=1, n_jobs=-1, refit='AUC')

    #grid_error = GridSearchCV(estimator=xgb_clf, param_grid=xgb_param_grid, scoring=scoring, cv=n_fold, verbose=1, n_jobs=-1, refit='AUC')

    grid_error.fit(X, y)

    # Print the best parameters and higher score
    y_pred = grid_error.predict(X_test)

    #Print summary
    print("<-- SUMMARY -->")
    print("")
    print("Model: XGBoost Classification")
    print("CV: Randomized Search Cross-Validation")
    print("Best parameters found: ", grid_error.best_params_)
    print("")
    print("")
    #Print classification report
    print("<-- CLASSIFICATION REPORT -->")
    print("")
    print(classification_report(y_test, y_pred))
    print("")
    print("")
    print('Best AUC Score: {}'.format(grid_error.best_score_))
    print('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))
    print("")
    print("")
    #Plotting confusion matrix
    print("<-- CONFUSION MATRIX -->")
    plot_confusion_matrix(y_test, y_pred)
    plt.show()
    print("")
    print("")

    #Values for the ROC Curve
    print("<-- ROC-AUC CURVE -->")
    y_pred_proba = grid_error.predict_proba(X_test)[::,1]
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    #Plotting the AUC Curve
    auc = roc_auc_score(y_test, y_pred_proba)
    #Create ROC curve
    plt.plot(fpr, tpr, label="AUC="+str(auc))
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    plt.title('ROC-AUC Curve')
    plt.legend(loc=4)
    plt.show()
    
    return (xgb_clf, grid_error)